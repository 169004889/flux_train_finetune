accelerate launch 
  --num_cpu_threads_per_process 8 flux_train_network.py \
  --pretrained_model_name_or_path "/本地路径/flux1-dev.safetensors" \
  --clip_l "/本地路径/clip_l.safetensors" \
  --t5xxl "/本地路径/t5xxl_fp8_e4m3fn.safetensors" \
  --ae "/本地路径/ae.safetensors" \
  --save_model_as safetensors \
  --sdpa \
  --persistent_data_loader_workers \
  --max_data_loader_n_workers 8 \
  --seed 2048 \
  --gradient_checkpointing \
  --mixed_precision bf16 \
  --save_precision bf16 \
  --network_module networks.lora_flux \
  --network_dim 16 \
  --network_alpha 8 \
  --learning_rate 1e-3 \
  --network_train_unet_only \
  --cache_latents_to_disk \
  --cache_text_encoder_outputs \
  --cache_text_encoder_outputs_to_disk \
  --highvram \
  --max_train_epochs 20 \
  --save_every_n_epochs 2 \
  --dataset_config "/本地路径/data_LoRA_config.toml" \
  --output_dir "/本地路径/模型保存地址" \
  --output_name "WeThnkIn_FLUX_LoRA" \
  --timestep_sampling shift \
  --discrete_flow_shift 3.1582 \
  --model_prediction_type raw \
  --guidance_scale 1.0 \
  --loss_type l2 \
  --optimizer_type adafactor \
  --optimizer_args "relative_step=False" "scale_parameter=False" "warmup_init=False" \
  --lr_scheduler constant_with_warmup \
  --max_grad_norm 0.0 \
  --t5xxl_max_token_length 512 \
  --fp8_base \
  --split_mode \
  --network_args "train_blocks=single"